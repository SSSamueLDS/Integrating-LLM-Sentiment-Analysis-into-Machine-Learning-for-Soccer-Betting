{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Calibration vs Accuracy Experiment</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Data and Splitting Into Training And Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do separately for each LLM\n",
    "#0 for deepseek, 1 for gpt4o-mini, 2 for qwen, 3 for openai 4.1\n",
    "selected_llm = 3 #change this to 1 for gpt4o-mini, 2 for qwen, 3 for openai 4.1\n",
    "llm = [\"deepseek\", \"openai\", \"qwen\", \"openai4.1\"]\n",
    "sentiment_list = [\"deepseek_home-away_sentiment\", \"openai_home-away_sentiment\", \"qwen_home-away_sentiment\", \"openai4.1_home-away_sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#Load sentiment JSON files\n",
    "with open('../dataset/epl_sentiment_analysis/sentiment_analysis_results_combined/combined_sentiment_2015_output.json', 'r') as f:\n",
    "    sentiment_2015 = json.load(f)\n",
    "with open('../dataset/epl_sentiment_analysis/sentiment_analysis_results_combined/combined_sentiment_2016_output.json', 'r') as f:\n",
    "    sentiment_2016 = json.load(f)\n",
    "with open('../dataset/epl_sentiment_analysis/sentiment_analysis_results_combined/combined_sentiment_2017_output.json', 'r') as f:\n",
    "    sentiment_2017 = json.load(f)\n",
    "with open('../dataset/epl_sentiment_analysis/sentiment_analysis_results_combined/combined_sentiment_2018_output.json', 'r') as f:\n",
    "    sentiment_2018 = json.load(f)\n",
    "with open('../dataset/epl_sentiment_analysis/sentiment_analysis_results_combined/combined_sentiment_2019_output.json', 'r') as f:\n",
    "    sentiment_2019 = json.load(f)\n",
    "with open('../dataset/epl_sentiment_analysis/sentiment_analysis_results_combined/combined_sentiment_2020_output.json', 'r') as f:\n",
    "    sentiment_2020 = json.load(f)\n",
    "with open('../dataset/epl_sentiment_analysis/sentiment_analysis_results_combined/combined_sentiment_2021_output.json', 'r') as f:\n",
    "    sentiment_2021 = json.load(f)\n",
    "with open('../dataset/epl_sentiment_analysis/sentiment_analysis_results_combined/combined_sentiment_2022_output.json', 'r') as f:\n",
    "    sentiment_2022 = json.load(f)\n",
    "with open('../dataset/epl_sentiment_analysis/sentiment_analysis_results_combined/combined_sentiment_2023_output.json', 'r') as f:\n",
    "    sentiment_2023 = json.load(f)\n",
    "\n",
    "# Normalize the sentiment data\n",
    "df_sentiment_2015 = pd.json_normalize(sentiment_2015)\n",
    "df_sentiment_2016 = pd.json_normalize(sentiment_2016)\n",
    "df_sentiment_2017 = pd.json_normalize(sentiment_2017)\n",
    "df_sentiment_2018 = pd.json_normalize(sentiment_2018)\n",
    "df_sentiment_2019 = pd.json_normalize(sentiment_2019)\n",
    "df_sentiment_2020 = pd.json_normalize(sentiment_2020)\n",
    "df_sentiment_2021 = pd.json_normalize(sentiment_2021)\n",
    "df_sentiment_2022 = pd.json_normalize(sentiment_2022)\n",
    "df_sentiment_2023 = pd.json_normalize(sentiment_2023)\n",
    "\n",
    "#leave only the sentiment for the selected LLM and the id\n",
    "df_sentiment_2015 = df_sentiment_2015[['id', sentiment_list[selected_llm]]]\n",
    "df_sentiment_2016 = df_sentiment_2016[['id', sentiment_list[selected_llm]]]\n",
    "df_sentiment_2017 = df_sentiment_2017[['id', sentiment_list[selected_llm]]]\n",
    "df_sentiment_2018 = df_sentiment_2018[['id', sentiment_list[selected_llm]]]\n",
    "df_sentiment_2019 = df_sentiment_2019[['id', sentiment_list[selected_llm]]]\n",
    "df_sentiment_2020 = df_sentiment_2020[['id', sentiment_list[selected_llm]]]\n",
    "df_sentiment_2021 = df_sentiment_2021[['id', sentiment_list[selected_llm]]]\n",
    "df_sentiment_2022 = df_sentiment_2022[['id', sentiment_list[selected_llm]]]\n",
    "df_sentiment_2023 = df_sentiment_2023[['id', sentiment_list[selected_llm]]]\n",
    "\n",
    "# Load the JSON file\n",
    "with open('../dataset/epl_combined/epl_combined_2015.json', 'r') as f:\n",
    "    data_2015 = json.load(f)\n",
    "\n",
    "with open('../dataset/epl_combined/epl_combined_2016.json', 'r') as f:\n",
    "    data_2016 = json.load(f)\n",
    "\n",
    "with open('../dataset/epl_combined/epl_combined_2017.json', 'r') as f:\n",
    "    data_2017 = json.load(f)\n",
    "\n",
    "with open('../dataset/epl_combined/epl_combined_2018.json', 'r') as f:\n",
    "    data_2018 = json.load(f)\n",
    "\n",
    "with open('../dataset/epl_combined/epl_combined_2019.json', 'r') as f:\n",
    "    data_2019 = json.load(f)\n",
    "\n",
    "with open('../dataset/epl_combined/epl_combined_2020.json', 'r') as f:\n",
    "    data_2020 = json.load(f)\n",
    "\n",
    "with open('../dataset/epl_combined/epl_combined_2021.json', 'r') as f:\n",
    "    data_2021 = json.load(f)\n",
    "\n",
    "with open('../dataset/epl_combined/epl_combined_2022.json', 'r') as f:\n",
    "    data_2022 = json.load(f)\n",
    "\n",
    "with open('../dataset/epl_combined/epl_combined_2023.json', 'r') as f:\n",
    "    data_2023 = json.load(f)\n",
    "\n",
    "# Normalize the data\n",
    "df_2015 = pd.json_normalize(data_2015)\n",
    "df_2016 = pd.json_normalize(data_2016)\n",
    "df_2017 = pd.json_normalize(data_2017)\n",
    "df_2018 = pd.json_normalize(data_2018)\n",
    "df_2019 = pd.json_normalize(data_2019)\n",
    "df_2020 = pd.json_normalize(data_2020)\n",
    "df_2021 = pd.json_normalize(data_2021)\n",
    "df_2022 = pd.json_normalize(data_2022)\n",
    "df_2023 = pd.json_normalize(data_2023) #unseen until betting simulation\n",
    "\n",
    "#join the sentiment data with the main data on 'id' in sentiment and 'fixture_id' in main data. remove any row with null values after the join\n",
    "df_2015 = df_2015.merge(df_sentiment_2015, left_on='fixture_id', right_on='id', how='left').dropna()\n",
    "df_2016 = df_2016.merge(df_sentiment_2016, left_on='fixture_id', right_on='id', how='left').dropna()\n",
    "df_2017 = df_2017.merge(df_sentiment_2017, left_on='fixture_id', right_on='id', how='left').dropna()\n",
    "df_2018 = df_2018.merge(df_sentiment_2018, left_on='fixture_id', right_on='id', how='left').dropna()\n",
    "df_2019 = df_2019.merge(df_sentiment_2019, left_on='fixture_id', right_on='id', how='left').dropna()\n",
    "df_2020 = df_2020.merge(df_sentiment_2020, left_on='fixture_id', right_on='id', how='left').dropna()\n",
    "df_2021 = df_2021.merge(df_sentiment_2021, left_on='fixture_id', right_on='id', how='left').dropna()\n",
    "df_2022 = df_2022.merge(df_sentiment_2022, left_on='fixture_id', right_on='id', how='left').dropna()\n",
    "df_2023 = df_2023.merge(df_sentiment_2023, left_on='fixture_id', right_on='id', how='left').dropna()\n",
    "\n",
    "df_2015 = df_2015.drop(columns=['id'])\n",
    "df_2016 = df_2016.drop(columns=['id'])\n",
    "df_2017 = df_2017.drop(columns=['id'])\n",
    "df_2018 = df_2018.drop(columns=['id'])\n",
    "df_2019 = df_2019.drop(columns=['id'])\n",
    "df_2020 = df_2020.drop(columns=['id'])\n",
    "df_2021 = df_2021.drop(columns=['id'])\n",
    "df_2022 = df_2022.drop(columns=['id'])\n",
    "df_2023 = df_2023.drop(columns=['id'])\n",
    "\n",
    "# Combine the data for 2015-2022\n",
    "df_feature_selection_tuning = pd.concat([df_2015, df_2016, df_2017, df_2018], ignore_index=True)\n",
    "df_train = pd.concat([df_feature_selection_tuning, df_2019, df_2020, df_2021], ignore_index=True)\n",
    "df_test = df_2022  # Test data\n",
    "df_final_train = pd.concat([df_train, df_test], ignore_index=True)  # Final train data\n",
    "df_betting_simulation = df_2023  # Betting simulation data\n",
    "\n",
    "# Drop unnecessary columns\n",
    "columns_to_drop = [\n",
    "    'fixture_id', 'timestamp', 'home_team', 'home_team_id', 'away_team', 'away_team_id',\n",
    "    'actual_home_goals', 'actual_away_goals', 'home_odds', 'away_odds', 'draw_odds'\n",
    "]\n",
    "\n",
    "df_feature_selection_tuning = df_feature_selection_tuning.drop(columns=columns_to_drop)\n",
    "df_train = df_train.drop(columns=columns_to_drop)\n",
    "df_test = df_test.drop(columns=columns_to_drop)\n",
    "df_final_train = df_final_train.drop(columns=columns_to_drop)\n",
    "df_betting_simulation = df_betting_simulation.drop(columns=columns_to_drop)\n",
    "\n",
    "# Split into X and y\n",
    "y_feature_selection_tuning = df_feature_selection_tuning['actual_winner']\n",
    "x_feature_selection_tuning = df_feature_selection_tuning.drop(columns=['actual_winner'])\n",
    "\n",
    "y_train = df_train['actual_winner']\n",
    "x_train = df_train.drop(columns=['actual_winner'])\n",
    "\n",
    "y_test = df_test['actual_winner']\n",
    "x_test = df_test.drop(columns=['actual_winner'])\n",
    "\n",
    "y_final_train = df_final_train['actual_winner']\n",
    "x_final_train = df_final_train.drop(columns=['actual_winner'])\n",
    "\n",
    "y_betting_simulation = df_betting_simulation['actual_winner']\n",
    "x_betting_simulation = df_betting_simulation.drop(columns=['actual_winner'])\n",
    "\n",
    "# Encode y values to numeric labels\n",
    "y_feature_selection_tuning = y_feature_selection_tuning.map({'home': 0, 'away': 1, 'draw': 2})\n",
    "y_train = y_train.map({'home': 0, 'away': 1, 'draw': 2})\n",
    "y_test = y_test.map({'home': 0, 'away': 1, 'draw': 2})\n",
    "y_final_train = y_final_train.map({'home': 0, 'away': 1, 'draw': 2})\n",
    "y_betting_simulation = y_betting_simulation.map({'home': 0, 'away': 1, 'draw': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_sentiment = x_feature_selection_tuning.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardising and Removing Highly Correlated Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: Index(['avg_diff_5_fulltime_score', 'avg_diff_5_shots_on_goal',\n",
      "       'avg_diff_5_shots_off_goal', 'avg_diff_5_total_shots',\n",
      "       'avg_diff_5_blocked_shots', 'avg_diff_5_shots_insidebox',\n",
      "       'avg_diff_5_shots_outsidebox', 'avg_diff_5_fouls',\n",
      "       'avg_diff_5_corner_kicks', 'avg_diff_5_offsides',\n",
      "       'avg_diff_5_ball_possession_%', 'avg_diff_5_yellow_cards',\n",
      "       'avg_diff_5_red_cards', 'avg_diff_5_goalkeeper_saves',\n",
      "       'avg_diff_5_passing_accuracy_%', 'avg_diff_fulltime_score',\n",
      "       'avg_diff_shots_on_goal', 'avg_diff_shots_off_goal',\n",
      "       'avg_diff_total_shots', 'avg_diff_blocked_shots',\n",
      "       'avg_diff_shots_insidebox', 'avg_diff_shots_outsidebox',\n",
      "       'avg_diff_fouls', 'avg_diff_corner_kicks', 'avg_diff_offsides',\n",
      "       'avg_diff_ball_possession_%', 'avg_diff_yellow_cards',\n",
      "       'avg_diff_red_cards', 'avg_diff_goalkeeper_saves',\n",
      "       'avg_diff_passing_accuracy_%', 'openai4.1_home-away_sentiment'],\n",
      "      dtype='object')\n",
      "number of features: 31\n"
     ]
    }
   ],
   "source": [
    "#standardise all features of x data frames such that the mean is 0 and the standard deviation is 1\n",
    "#retain the column names\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_feature_selection_tuning)\n",
    "x_train = pd.DataFrame(scaler.transform(x_train), columns=x_train.columns)\n",
    "x_feature_selection_tuning = pd.DataFrame(scaler.transform(x_feature_selection_tuning), columns=x_feature_selection_tuning.columns)\n",
    "x_test = pd.DataFrame(scaler.transform(x_test), columns=x_test.columns)\n",
    "x_final_train = pd.DataFrame(scaler.transform(x_final_train), columns=x_final_train.columns)\n",
    "x_betting_simulation = pd.DataFrame(scaler.transform(x_betting_simulation), columns=x_betting_simulation.columns)\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "Two features are considered highly correlated if their spearman correlation coefficient is greater than 0.7 Xiao et al. (2016).\n",
    " For a given group of highly correlated features, we consider all except the feature which is most correlated with the target to be redundant, \n",
    " and so remove them.\n",
    "\n",
    " \"\"\"\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "def remove_redundant_features(X):\n",
    "    \"\"\"\n",
    "    Remove redundant features based on Spearman correlation criteria without target consideration.\n",
    "    \n",
    "    Parameters:\n",
    "    X (pd.DataFrame): Input features DataFrame\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with redundant features removed\n",
    "    \"\"\"\n",
    "    # Compute feature-feature Spearman correlation matrix\n",
    "    corr_matrix = X.corr(method='spearman')\n",
    "    \n",
    "    # Create graph of features with correlation > 0.7\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(corr_matrix.columns)\n",
    "    features = corr_matrix.columns\n",
    "    for i in range(len(features)):\n",
    "        for j in range(i + 1, len(features)):\n",
    "            if abs(corr_matrix.iloc[i, j]) > 0.7:\n",
    "                G.add_edge(features[i], features[j])\n",
    "    \n",
    "    # Find connected components (groups of correlated features)\n",
    "    connected_components = list(nx.connected_components(G))\n",
    "    \n",
    "    # Identify features to remove\n",
    "    to_remove = set()\n",
    "    for group in connected_components:\n",
    "        if len(group) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Convert set to list for consistent ordering\n",
    "        group_list = sorted(list(group))\n",
    "        \n",
    "        # Keep the first feature and remove others (alternative: keep feature with highest variance)\n",
    "        # To use highest variance instead, replace next line with:\n",
    "        # keep_feature = X[group_list].var().idxmax()\n",
    "        keep_feature = group_list[0]\n",
    "        \n",
    "        to_remove.update(feature for feature in group_list if feature != keep_feature)\n",
    "    \n",
    "    return X.drop(columns=list(to_remove))\n",
    "\n",
    "#remobe the llm sentiment column from the feature selection tuning data frame\n",
    "#make a copy for tuning before removing the column\n",
    "x_tuning  =  x_feature_selection_tuning.copy()\n",
    "#remove the sentiment column from the feature selection tuning data frame\n",
    "x_feature_selection_tuning = x_feature_selection_tuning.drop(columns=[sentiment_list[selected_llm]])\n",
    "\n",
    "x_feature_selection_tuning = remove_redundant_features(x_feature_selection_tuning)\n",
    "\n",
    "#drop all the columns that are not in x_feature_selection_tuning but not the sentiment column\n",
    "\n",
    "print(f'features: {x_train.columns}')\n",
    "print(f'number of features: {len(x_train.columns)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Feature Selection</h1>\n",
    "<ol>\n",
    "<li>Sequential forward selection (SFS)</li>\n",
    "<li>Logistic Regression as the learning algorithm</li>\n",
    "<li>Objective function is the classwise ECE for the calibration branch, while accuracy is the objective function for the accuracy branch</li>\n",
    "<li>The input to the process is feature subset A</li>\n",
    "<li>Must define training set and evaluation set. Cross validation, with 25% of the dataset used at any one time</li>\n",
    "\n",
    "<li>For the calibration\n",
    "branch, the feature subset under which the LR model achieves\n",
    "the lowest classwise-ECE (fitted to an initial training set and\n",
    "evaluated on a validation set) is considered the optimal feature\n",
    "subset for calibration and is denoted as subset B. </li>\n",
    "<li>For the accuracy branch, the feature subset under which\n",
    "the highest accuracy is achieved is considered optimal and is\n",
    "denoted as subset C.</li>\n",
    "<li>For calibration, there must be at least 16 out of 20 bins filled</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validated accuracy: 0.5295857817720936\n",
      "Calibration features: ['avg_diff_goalkeeper_saves', 'avg_diff_yellow_cards', 'avg_diff_red_cards', 'avg_diff_fouls', 'openai4.1_home-away_sentiment']\n",
      "Number of calibration features: 5\n",
      "Accuracy features: ['avg_diff_5_ball_possession_%', 'avg_diff_offsides', 'avg_diff_5_red_cards', 'avg_diff_5_fouls', 'avg_diff_goalkeeper_saves', 'avg_diff_5_yellow_cards', 'avg_diff_red_cards', 'avg_diff_5_offsides', 'openai4.1_home-away_sentiment']\n",
      "Number of accuracy features: 9\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def expected_calibration_error(samples=None, true_labels=None, confidences=None, accuracies=None, M=10):\n",
    "    if samples is not None and true_labels is not None:\n",
    "        confidences = np.max(samples, axis=1)\n",
    "        predicted_label = np.argmax(samples, axis=1)\n",
    "        accuracies = predicted_label == true_labels\n",
    "    elif confidences is not None and accuracies is not None:\n",
    "        if len(confidences) != len(accuracies):\n",
    "            raise ValueError(\"confidences and accuracies must have the same length.\")\n",
    "    else:\n",
    "        raise ValueError(\"Either (samples and true_labels) or (confidences and accuracies) must be provided.\")\n",
    "    \n",
    "    bin_boundaries = np.linspace(0, 1, M + 1)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "\n",
    "    ece = 0.0\n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        in_bin = np.logical_and(confidences > bin_lower, confidences <= bin_upper)\n",
    "        prob_in_bin = np.mean(in_bin)\n",
    "        if prob_in_bin > 0:\n",
    "            accuracy_in_bin = np.mean(accuracies[in_bin])\n",
    "            avg_confidence_in_bin = np.mean(confidences[in_bin])\n",
    "            ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prob_in_bin\n",
    "    return ece\n",
    "\n",
    "def select_calibration_features(X, y):\n",
    "    current_features = []\n",
    "    remaining_features = X.columns.tolist()\n",
    "    best_ece = float('inf')\n",
    "    M_bins = 10\n",
    "    min_filled_bins = 5\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    while remaining_features:\n",
    "        candidates = []\n",
    "        for feature in remaining_features:\n",
    "            candidate_features = current_features + [feature]\n",
    "            X_subset = X[candidate_features]\n",
    "            \n",
    "            all_confidences = []\n",
    "            all_accuracies = []\n",
    "            \n",
    "            for train_idx, val_idx in kf.split(X_subset):\n",
    "                X_train_fold = X_subset.iloc[train_idx]\n",
    "                X_val_fold = X_subset.iloc[val_idx]\n",
    "                y_train_fold = y.iloc[train_idx]\n",
    "                y_val_fold = y.iloc[val_idx]\n",
    "                \n",
    "                lr = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "                lr.fit(X_train_fold, y_train_fold)\n",
    "                y_probs = lr.predict_proba(X_val_fold)\n",
    "                \n",
    "                fold_confidences = np.max(y_probs, axis=1)\n",
    "                fold_accuracies = (np.argmax(y_probs, axis=1) == y_val_fold.to_numpy())\n",
    "                \n",
    "                all_confidences.extend(fold_confidences)\n",
    "                all_accuracies.extend(fold_accuracies)\n",
    "            \n",
    "            all_confidences = np.array(all_confidences)\n",
    "            all_accuracies = np.array(all_accuracies)\n",
    "            \n",
    "            filled_bins = sum(\n",
    "                np.logical_and(all_confidences > (i/M_bins), all_confidences <= ((i+1)/M_bins)).any()\n",
    "                for i in range(M_bins)\n",
    "            )\n",
    "            \n",
    "            if filled_bins >= min_filled_bins:\n",
    "                ece = expected_calibration_error(confidences=all_confidences, accuracies=all_accuracies, M=M_bins)\n",
    "                candidates.append((ece, feature))\n",
    "        \n",
    "        if not candidates:\n",
    "            break\n",
    "            \n",
    "        candidates.sort(key=lambda x: x[0])\n",
    "        best_ece_candidate, best_feature = candidates[0]\n",
    "        \n",
    "        if best_ece_candidate < best_ece:\n",
    "            best_ece = best_ece_candidate\n",
    "            current_features.append(best_feature)\n",
    "            remaining_features.remove(best_feature)\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return current_features\n",
    "\n",
    "def select_accuracy_features(X, y):\n",
    "    features = X.columns.tolist()\n",
    "    remaining_features = features.copy()\n",
    "    selected_features = []\n",
    "    best_accuracy = 0.0\n",
    "    best_feature_set = []\n",
    "    TOLERANCE = 0.001\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    while remaining_features:\n",
    "        current_best_accuracy = best_accuracy - TOLERANCE\n",
    "        current_best_feature = None\n",
    "        \n",
    "        for feature in remaining_features:\n",
    "            candidate_features = selected_features + [feature]\n",
    "            X_subset = X[candidate_features]\n",
    "            \n",
    "            fold_accuracies = []\n",
    "            for train_idx, val_idx in kf.split(X_subset):\n",
    "                X_train_fold = X_subset.iloc[train_idx]\n",
    "                X_val_fold = X_subset.iloc[val_idx]\n",
    "                y_train_fold = y.iloc[train_idx]\n",
    "                y_val_fold = y.iloc[val_idx]\n",
    "                \n",
    "                lr = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "                lr.fit(X_train_fold, y_train_fold)\n",
    "                y_pred = lr.predict(X_val_fold)\n",
    "                acc = accuracy_score(y_val_fold, y_pred)\n",
    "                fold_accuracies.append(acc)\n",
    "            \n",
    "            avg_acc = np.mean(fold_accuracies)\n",
    "            \n",
    "            if avg_acc > current_best_accuracy:\n",
    "                current_best_accuracy = avg_acc\n",
    "                current_best_feature = feature\n",
    "        \n",
    "        if current_best_feature:\n",
    "            selected_features.append(current_best_feature)\n",
    "            remaining_features.remove(current_best_feature)\n",
    "            best_accuracy = current_best_accuracy\n",
    "            best_feature_set = selected_features.copy()\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    print(f'Best cross-validated accuracy: {best_accuracy}')\n",
    "    return best_feature_set\n",
    "\n",
    "def feature_selection(input_df, input_labels):\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(input_labels)\n",
    "    y = pd.Series(y_encoded)\n",
    "    \n",
    "    calibration_features = select_calibration_features(input_df, y)\n",
    "    accuracy_features = select_accuracy_features(input_df, y)\n",
    "    \n",
    "    return calibration_features, accuracy_features\n",
    "\n",
    "calib_features, acc_features = feature_selection(x_feature_selection_tuning, y_feature_selection_tuning)\n",
    "stats_feature = acc_features.copy()\n",
    "\n",
    "if sentiment_list[selected_llm] not in calib_features:\n",
    "    calib_features.append(sentiment_list[selected_llm])\n",
    "if sentiment_list[selected_llm] not in acc_features:\n",
    "    acc_features.append(sentiment_list[selected_llm])\n",
    "\n",
    "\n",
    "\n",
    "print(f'Calibration features: {calib_features}')\n",
    "print(f'Number of calibration features: {len(calib_features)}')\n",
    "print(f'Accuracy features: {acc_features}')\n",
    "print(f'Number of accuracy features: {len(acc_features)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added GPT-4.1 sentiment feature: openai4.1_home-away_sentiment\n",
      "Final x_llm shape: (1318, 9)\n",
      "Final x_llm columns: ['avg_diff_5_ball_possession_%', 'avg_diff_offsides', 'avg_diff_5_red_cards', 'avg_diff_5_fouls', 'avg_diff_goalkeeper_saves', 'avg_diff_5_yellow_cards', 'avg_diff_red_cards', 'avg_diff_5_offsides', 'openai4.1_home-away_sentiment']\n"
     ]
    }
   ],
   "source": [
    "x_llm = x_feature_selection_tuning[stats_feature].copy()\n",
    "sentiment_list = [\"deepseek_home-away_sentiment\", \"openai_home-away_sentiment\", \"qwen_home-away_sentiment\", \"openai4.1_home-away_sentiment\"]\n",
    "\n",
    "llm_sentiment\n",
    "\n",
    "if selected_llm == 0:  # DeepSeek-R1\n",
    "    x_llm[sentiment_list[0]] = llm_sentiment[sentiment_list[0]]\n",
    "    print(f\"Added DeepSeek-R1 sentiment feature: {sentiment_list[0]}\")\n",
    "    \n",
    "elif selected_llm == 1:  # GPT-4o mini\n",
    "    x_llm[sentiment_list[1]] = llm_sentiment[sentiment_list[1]]\n",
    "    print(f\"Added GPT-4o mini sentiment feature: {sentiment_list[1]}\")\n",
    "    \n",
    "elif selected_llm == 2:  # Qwen2.5-Plus\n",
    "    x_llm[sentiment_list[2]] = llm_sentiment[sentiment_list[2]]\n",
    "    print(f\"Added Qwen2.5-Plus sentiment feature: {sentiment_list[2]}\")\n",
    "    \n",
    "else:  # selected_llm == 3, GPT-4.1\n",
    "    x_llm[sentiment_list[3]] = llm_sentiment[sentiment_list[3]]\n",
    "    print(f\"Added GPT-4.1 sentiment feature: {sentiment_list[3]}\")\n",
    "\n",
    "print(f\"Final x_llm shape: {x_llm.shape}\")\n",
    "print(f\"Final x_llm columns: {x_llm.columns.tolist()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Hyperparameter Tuning of Models using Grid Search</h1>\n",
    "<ol>\n",
    "<li>Logistic regression</li>\n",
    "<li>Random Forest</li>\n",
    "<li>SVC (Support Vector Classification)</li>\n",
    "<li>XGBoost</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\samue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\samue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\samue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from xgboost) (1.16.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running optimization for logistic_regression...\n",
      "Model: logistic_regression, F1 Score: 0.4723, Log Loss: 0.9882\n",
      "Model: logistic_regression, F1 Score: 0.4683, Log Loss: 0.9857\n",
      "Model: logistic_regression, F1 Score: 0.4656, Log Loss: 0.9789\n",
      "Model: logistic_regression, F1 Score: 0.4841, Log Loss: 0.9869\n",
      "Model: logistic_regression, F1 Score: 0.4724, Log Loss: 0.9863\n",
      "Model: logistic_regression, F1 Score: 0.4723, Log Loss: 0.9885\n",
      "Model: logistic_regression, F1 Score: 0.4723, Log Loss: 0.9886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\samue\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\samue\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: logistic_regression, F1 Score: 0.4652, Log Loss: 0.9869\n",
      "Model: logistic_regression, F1 Score: 0.4723, Log Loss: 0.9886\n",
      "Model: logistic_regression, F1 Score: 0.4913, Log Loss: 0.9983\n",
      "Model: logistic_regression, F1 Score: 0.4752, Log Loss: 0.9884\n",
      "Model: logistic_regression, F1 Score: 0.4652, Log Loss: 0.9868\n",
      "Model: logistic_regression, F1 Score: 0.4723, Log Loss: 0.9886\n",
      "Model: logistic_regression, F1 Score: 0.4727, Log Loss: 0.9859\n",
      "Model: logistic_regression, F1 Score: 0.4652, Log Loss: 0.9869\n",
      "Model: logistic_regression, F1 Score: 0.4707, Log Loss: 0.9863\n",
      "Model: logistic_regression, F1 Score: 0.4723, Log Loss: 0.9885\n",
      "Model: logistic_regression, F1 Score: 0.4732, Log Loss: 0.9868\n",
      "Model: logistic_regression, F1 Score: 0.4723, Log Loss: 0.9886\n",
      "Model: logistic_regression, F1 Score: 0.4652, Log Loss: 0.9868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\samue\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\samue\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: logistic_regression, F1 Score: 0.4723, Log Loss: 0.9886\n",
      "Model: logistic_regression, F1 Score: 0.4841, Log Loss: 0.9869\n",
      "Model: logistic_regression, F1 Score: 0.4723, Log Loss: 0.9885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\samue\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\samue\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: logistic_regression, F1 Score: 0.4723, Log Loss: 0.9886\n",
      "Model: logistic_regression, F1 Score: 0.4723, Log Loss: 0.9885\n",
      "Model: logistic_regression, F1 Score: 0.4762, Log Loss: 0.9841\n",
      "Model: logistic_regression, F1 Score: 0.4893, Log Loss: 0.9891\n",
      "Model: logistic_regression, F1 Score: 0.4841, Log Loss: 0.9869\n",
      "Model: logistic_regression, F1 Score: 0.4671, Log Loss: 0.9844\n",
      "Model: logistic_regression, F1 Score: 0.4630, Log Loss: 0.9817\n",
      "Model: logistic_regression, F1 Score: 0.4841, Log Loss: 0.9869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\samue\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\samue\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: logistic_regression, F1 Score: 0.4723, Log Loss: 0.9886\n",
      "Model: logistic_regression, F1 Score: 0.4781, Log Loss: 0.9876\n",
      "Model: logistic_regression, F1 Score: 0.4723, Log Loss: 0.9886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\samue\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\samue\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: logistic_regression, F1 Score: 0.4723, Log Loss: 0.9886\n",
      "Model: logistic_regression, F1 Score: 0.4841, Log Loss: 0.9870\n",
      "Model: logistic_regression, F1 Score: 0.4841, Log Loss: 0.9870\n",
      "Model: logistic_regression, F1 Score: 0.4737, Log Loss: 0.9840\n",
      "Model: logistic_regression, F1 Score: 0.4723, Log Loss: 0.9885\n",
      "Model: logistic_regression, F1 Score: 0.4588, Log Loss: 0.9852\n",
      "Model: logistic_regression, F1 Score: 0.4882, Log Loss: 0.9877\n",
      "Model: logistic_regression, F1 Score: 0.4873, Log Loss: 0.9907\n",
      "Model: logistic_regression, F1 Score: 0.4671, Log Loss: 0.9842\n",
      "Model: logistic_regression, F1 Score: 0.4692, Log Loss: 0.9868\n",
      "Model: logistic_regression, F1 Score: 0.4723, Log Loss: 0.9886\n",
      "Model: logistic_regression, F1 Score: 0.4781, Log Loss: 0.9874\n",
      "Best hyperparameters for logistic_regression: ['l2', 0.01, 'liblinear', 0.0]\n",
      "Best F1 score for logistic_regression: 0.4913\n",
      "\n",
      "\n",
      "Running optimization for svc...\n",
      "Model: svc, F1 Score: 0.4427, Log Loss: 1.0121\n",
      "Model: svc, F1 Score: 0.4618, Log Loss: 1.0143\n",
      "Model: svc, F1 Score: 0.4596, Log Loss: 0.9971\n",
      "Model: svc, F1 Score: 0.4427, Log Loss: 1.0119\n",
      "Model: svc, F1 Score: 0.4414, Log Loss: 1.0109\n",
      "Model: svc, F1 Score: 0.4488, Log Loss: 1.0213\n",
      "Model: svc, F1 Score: 0.4610, Log Loss: 0.9979\n",
      "Model: svc, F1 Score: 0.4610, Log Loss: 0.9977\n",
      "Model: svc, F1 Score: 0.4610, Log Loss: 0.9925\n",
      "Model: svc, F1 Score: 0.4580, Log Loss: 1.0192\n",
      "Model: svc, F1 Score: 0.4473, Log Loss: 1.0117\n",
      "Model: svc, F1 Score: 0.4610, Log Loss: 0.9932\n",
      "Model: svc, F1 Score: 0.4624, Log Loss: 0.9961\n",
      "Model: svc, F1 Score: 0.4610, Log Loss: 0.9961\n",
      "Model: svc, F1 Score: 0.4471, Log Loss: 1.0246\n",
      "Model: svc, F1 Score: 0.4610, Log Loss: 1.0019\n",
      "Model: svc, F1 Score: 0.4624, Log Loss: 0.9935\n",
      "Model: svc, F1 Score: 0.4596, Log Loss: 0.9981\n",
      "Model: svc, F1 Score: 0.4610, Log Loss: 1.0005\n",
      "Model: svc, F1 Score: 0.4335, Log Loss: 1.0230\n",
      "Model: svc, F1 Score: 0.4289, Log Loss: 1.0225\n",
      "Model: svc, F1 Score: 0.4610, Log Loss: 1.0019\n",
      "Model: svc, F1 Score: 0.4624, Log Loss: 0.9994\n",
      "Model: svc, F1 Score: 0.4436, Log Loss: 1.0093\n",
      "Model: svc, F1 Score: 0.4610, Log Loss: 0.9996\n",
      "Model: svc, F1 Score: 0.4610, Log Loss: 0.9938\n",
      "Model: svc, F1 Score: 0.4596, Log Loss: 0.9976\n",
      "Model: svc, F1 Score: 0.4716, Log Loss: 1.0122\n",
      "Model: svc, F1 Score: 0.4693, Log Loss: 1.0129\n",
      "Model: svc, F1 Score: 0.4643, Log Loss: 1.0136\n",
      "Model: svc, F1 Score: 0.4610, Log Loss: 0.9982\n",
      "Model: svc, F1 Score: 0.4358, Log Loss: 1.0227\n",
      "Model: svc, F1 Score: 0.4596, Log Loss: 0.9974\n",
      "Model: svc, F1 Score: 0.4581, Log Loss: 1.0117\n",
      "Model: svc, F1 Score: 0.4708, Log Loss: 1.0133\n",
      "Model: svc, F1 Score: 0.4610, Log Loss: 0.9933\n",
      "Model: svc, F1 Score: 0.4708, Log Loss: 1.0133\n",
      "Model: svc, F1 Score: 0.4708, Log Loss: 1.0133\n",
      "Model: svc, F1 Score: 0.4708, Log Loss: 1.0133\n",
      "Model: svc, F1 Score: 0.4708, Log Loss: 1.0133\n",
      "Model: svc, F1 Score: 0.4700, Log Loss: 1.0132\n",
      "Model: svc, F1 Score: 0.4582, Log Loss: 0.9974\n",
      "Model: svc, F1 Score: 0.4596, Log Loss: 0.9944\n",
      "Model: svc, F1 Score: 0.4700, Log Loss: 1.0132\n",
      "Model: svc, F1 Score: 0.4610, Log Loss: 0.9995\n",
      "Model: svc, F1 Score: 0.4580, Log Loss: 1.0194\n",
      "Model: svc, F1 Score: 0.4596, Log Loss: 0.9910\n",
      "Model: svc, F1 Score: 0.4658, Log Loss: 1.0145\n",
      "Model: svc, F1 Score: 0.4610, Log Loss: 0.9927\n",
      "Model: svc, F1 Score: 0.4465, Log Loss: 1.0201\n",
      "Best hyperparameters for svc: ['rbf', 0.505929819985789, 'scale']\n",
      "Best F1 score for svc: 0.4716\n",
      "\n",
      "\n",
      "Running optimization for random_forest...\n",
      "Model: random_forest, F1 Score: 0.4653, Log Loss: 0.9945\n",
      "Model: random_forest, F1 Score: 0.4646, Log Loss: 1.0192\n",
      "Model: random_forest, F1 Score: 0.4743, Log Loss: 1.0086\n",
      "Model: random_forest, F1 Score: 0.4602, Log Loss: 0.9943\n",
      "Model: random_forest, F1 Score: 0.4470, Log Loss: 0.9968\n",
      "Model: random_forest, F1 Score: 0.4415, Log Loss: 1.0225\n",
      "Model: random_forest, F1 Score: 0.4476, Log Loss: 1.0901\n",
      "Model: random_forest, F1 Score: 0.4618, Log Loss: 1.4670\n",
      "Model: random_forest, F1 Score: 0.4815, Log Loss: 1.0365\n",
      "Model: random_forest, F1 Score: 0.4656, Log Loss: 1.0167\n",
      "Model: random_forest, F1 Score: 0.4618, Log Loss: 1.4401\n",
      "Model: random_forest, F1 Score: 0.4407, Log Loss: 1.0290\n",
      "Model: random_forest, F1 Score: 0.4800, Log Loss: 1.1667\n",
      "Model: random_forest, F1 Score: 0.4675, Log Loss: 1.0092\n",
      "Model: random_forest, F1 Score: 0.4751, Log Loss: 1.0088\n",
      "Model: random_forest, F1 Score: 0.4558, Log Loss: 1.1868\n",
      "Model: random_forest, F1 Score: 0.4571, Log Loss: 1.0107\n",
      "Model: random_forest, F1 Score: 0.4649, Log Loss: 1.0161\n",
      "Model: random_forest, F1 Score: 0.4646, Log Loss: 0.9945\n",
      "Model: random_forest, F1 Score: 0.4626, Log Loss: 0.9954\n",
      "Model: random_forest, F1 Score: 0.4669, Log Loss: 1.0177\n",
      "Model: random_forest, F1 Score: 0.4626, Log Loss: 1.0347\n",
      "Model: random_forest, F1 Score: 0.4709, Log Loss: 1.0069\n",
      "Model: random_forest, F1 Score: 0.4550, Log Loss: 1.1852\n",
      "Model: random_forest, F1 Score: 0.4643, Log Loss: 1.0166\n",
      "Model: random_forest, F1 Score: 0.4726, Log Loss: 1.0174\n",
      "Model: random_forest, F1 Score: 0.4735, Log Loss: 1.0171\n",
      "Model: random_forest, F1 Score: 0.4636, Log Loss: 1.0202\n",
      "Model: random_forest, F1 Score: 0.4665, Log Loss: 1.0113\n",
      "Model: random_forest, F1 Score: 0.4763, Log Loss: 1.0074\n",
      "Model: random_forest, F1 Score: 0.4715, Log Loss: 1.0084\n",
      "Model: random_forest, F1 Score: 0.4481, Log Loss: 1.0086\n",
      "Model: random_forest, F1 Score: 0.4693, Log Loss: 1.0204\n",
      "Model: random_forest, F1 Score: 0.4762, Log Loss: 1.0171\n",
      "Model: random_forest, F1 Score: 0.4670, Log Loss: 1.0215\n",
      "Model: random_forest, F1 Score: 0.4477, Log Loss: 1.0158\n",
      "Model: random_forest, F1 Score: 0.4690, Log Loss: 0.9941\n",
      "Model: random_forest, F1 Score: 0.4735, Log Loss: 1.0192\n",
      "Model: random_forest, F1 Score: 0.4727, Log Loss: 1.0189\n",
      "Model: random_forest, F1 Score: 0.4703, Log Loss: 1.0192\n",
      "Model: random_forest, F1 Score: 0.4816, Log Loss: 1.0151\n",
      "Model: random_forest, F1 Score: 0.4688, Log Loss: 1.0162\n",
      "Model: random_forest, F1 Score: 0.4760, Log Loss: 1.0177\n",
      "Model: random_forest, F1 Score: 0.4810, Log Loss: 1.0158\n",
      "Model: random_forest, F1 Score: 0.4640, Log Loss: 1.0197\n",
      "Model: random_forest, F1 Score: 0.4510, Log Loss: 1.0168\n",
      "Model: random_forest, F1 Score: 0.4522, Log Loss: 1.0157\n",
      "Model: random_forest, F1 Score: 0.4729, Log Loss: 1.0176\n",
      "Model: random_forest, F1 Score: 0.4727, Log Loss: 1.0177\n",
      "Model: random_forest, F1 Score: 0.4691, Log Loss: 1.0080\n",
      "Best hyperparameters for random_forest: [173, 10, 'gini']\n",
      "Best F1 score for random_forest: 0.4816\n",
      "\n",
      "\n",
      "Running optimization for xgboost...\n",
      "Model: xgboost, F1 Score: 0.4773, Log Loss: 1.0214\n",
      "Model: xgboost, F1 Score: 0.4781, Log Loss: 1.0079\n",
      "Model: xgboost, F1 Score: 0.4612, Log Loss: 1.0450\n",
      "Model: xgboost, F1 Score: 0.4728, Log Loss: 1.0125\n",
      "Model: xgboost, F1 Score: 0.4660, Log Loss: 1.0477\n",
      "Model: xgboost, F1 Score: 0.4955, Log Loss: 0.9996\n",
      "Model: xgboost, F1 Score: 0.4921, Log Loss: 1.0029\n",
      "Model: xgboost, F1 Score: 0.4842, Log Loss: 1.0011\n",
      "Model: xgboost, F1 Score: 0.4711, Log Loss: 1.0519\n",
      "Model: xgboost, F1 Score: 0.4953, Log Loss: 1.0032\n",
      "Model: xgboost, F1 Score: 0.4867, Log Loss: 0.9991\n",
      "Model: xgboost, F1 Score: 0.4932, Log Loss: 0.9999\n",
      "Model: xgboost, F1 Score: 0.4770, Log Loss: 1.0087\n",
      "Model: xgboost, F1 Score: 0.4797, Log Loss: 1.0072\n",
      "Model: xgboost, F1 Score: 0.4973, Log Loss: 0.9997\n",
      "Model: xgboost, F1 Score: 0.4902, Log Loss: 1.0359\n",
      "Model: xgboost, F1 Score: 0.4947, Log Loss: 1.0292\n",
      "Model: xgboost, F1 Score: 0.4878, Log Loss: 1.0209\n",
      "Model: xgboost, F1 Score: 0.4905, Log Loss: 1.0182\n",
      "Model: xgboost, F1 Score: 0.4983, Log Loss: 1.0033\n",
      "Model: xgboost, F1 Score: 0.4809, Log Loss: 1.0196\n",
      "Model: xgboost, F1 Score: 0.4799, Log Loss: 1.0310\n",
      "Model: xgboost, F1 Score: 0.4902, Log Loss: 1.0112\n",
      "Model: xgboost, F1 Score: 0.4916, Log Loss: 1.0004\n",
      "Model: xgboost, F1 Score: 0.4927, Log Loss: 1.0154\n",
      "Model: xgboost, F1 Score: 0.5020, Log Loss: 1.0161\n",
      "Model: xgboost, F1 Score: 0.4986, Log Loss: 1.0215\n",
      "Model: xgboost, F1 Score: 0.4820, Log Loss: 1.0180\n",
      "Model: xgboost, F1 Score: 0.4895, Log Loss: 1.0164\n",
      "Model: xgboost, F1 Score: 0.4770, Log Loss: 1.0057\n",
      "Model: xgboost, F1 Score: 0.4870, Log Loss: 1.0014\n",
      "Model: xgboost, F1 Score: 0.5006, Log Loss: 1.0285\n",
      "Model: xgboost, F1 Score: 0.4827, Log Loss: 1.0167\n",
      "Model: xgboost, F1 Score: 0.4688, Log Loss: 1.0735\n",
      "Model: xgboost, F1 Score: 0.4922, Log Loss: 1.0170\n",
      "Model: xgboost, F1 Score: 0.4875, Log Loss: 1.0307\n",
      "Model: xgboost, F1 Score: 0.4825, Log Loss: 1.0382\n",
      "Model: xgboost, F1 Score: 0.4903, Log Loss: 1.0408\n",
      "Model: xgboost, F1 Score: 0.4889, Log Loss: 1.0033\n",
      "Model: xgboost, F1 Score: 0.4941, Log Loss: 1.0414\n",
      "Model: xgboost, F1 Score: 0.4902, Log Loss: 1.0387\n",
      "Model: xgboost, F1 Score: 0.4891, Log Loss: 1.0024\n",
      "Model: xgboost, F1 Score: 0.4492, Log Loss: 1.0869\n",
      "Model: xgboost, F1 Score: 0.4905, Log Loss: 1.0387\n",
      "Model: xgboost, F1 Score: 0.4715, Log Loss: 1.0335\n",
      "Model: xgboost, F1 Score: 0.4717, Log Loss: 1.0355\n",
      "Model: xgboost, F1 Score: 0.4885, Log Loss: 1.0010\n",
      "Model: xgboost, F1 Score: 0.4879, Log Loss: 0.9969\n",
      "Model: xgboost, F1 Score: 0.4827, Log Loss: 1.0402\n",
      "Model: xgboost, F1 Score: 0.4826, Log Loss: 1.0000\n",
      "Best hyperparameters for xgboost: [98, 5, 0.01, 0.8110494938821138, 0.1, 0.022437548124240977, 0.491742378802224]\n",
      "Best F1 score for xgboost: 0.5020\n",
      "\n",
      "\n",
      "Evaluating initial logistic_regression model...\n",
      "Training - F1 Score: 0.4675, Accuracy: 0.5397, Log Loss: 0.9895\n",
      "Testing - F1 Score: 0.4666, Accuracy: 0.5364, Log Loss: 0.9991\n",
      "\n",
      "Evaluating initial svc model...\n",
      "Training - F1 Score: 0.4725, Accuracy: 0.5531, Log Loss: 0.9532\n",
      "Testing - F1 Score: 0.4306, Accuracy: 0.5061, Log Loss: 1.0177\n",
      "\n",
      "Evaluating initial random_forest model...\n",
      "Training - F1 Score: 0.8414, Accuracy: 0.8451, Log Loss: 0.6064\n",
      "Testing - F1 Score: 0.4325, Accuracy: 0.4939, Log Loss: 1.0208\n",
      "\n",
      "Evaluating initial xgboost model...\n",
      "Training - F1 Score: 0.5513, Accuracy: 0.6039, Log Loss: 0.9482\n",
      "Testing - F1 Score: 0.4386, Accuracy: 0.5030, Log Loss: 1.0271\n",
      "\n",
      "Training final logistic_regression model...\n",
      "Final logistic_regression model trained.\n",
      "\n",
      "Training final svc model...\n",
      "Final svc model trained.\n",
      "\n",
      "Training final random_forest model...\n",
      "Final random_forest model trained.\n",
      "\n",
      "Training final xgboost model...\n",
      "Final xgboost model trained.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt import gp_minimize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score, log_loss, accuracy_score  # Added: imported accuracy_score\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Define search spaces\n",
    "logistic_regression_space = [\n",
    "    Categorical([\"none\", \"l1\", \"l2\", \"elasticnet\"], name=\"penalty\"),\n",
    "    Real(0.01, 10, name=\"C\"),\n",
    "    Categorical([\"liblinear\", \"saga\"], name=\"solver\"),\n",
    "    Real(0.0, 1.0, name=\"l1_ratio\"),\n",
    "]\n",
    "\n",
    "svc_space = [\n",
    "    Categorical([\"linear\", \"rbf\"], name=\"kernel\"), \n",
    "    Real(0.1, 10, name=\"C\", prior='log-uniform'),   \n",
    "    Categorical([\"scale\", \"auto\"], name=\"gamma\"),   \n",
    "]\n",
    "random_forest_space = [\n",
    "    Integer(10, 200, name=\"n_estimators\"),\n",
    "    Integer(2, 20, name=\"max_depth\"),\n",
    "    Categorical([\"gini\", \"entropy\"], name=\"criterion\"),\n",
    "]\n",
    "\n",
    "xgboost_space = [\n",
    "    Integer(50, 100, name='n_estimators'),\n",
    "    Integer(3, 5, name='max_depth'),\n",
    "    Real(0.01, 0.1, name='learning_rate'),\n",
    "    Real(0.7, 0.9, name='subsample'),\n",
    "    Real(0.05, 0.1, name='gamma'),\n",
    "    Real(0, 1, name='reg_lambda'),\n",
    "    Real(0, 1, name='reg_alpha'),\n",
    "]\n",
    "\n",
    "# Define the objective function\n",
    "def objective_function(params, model_type, X_train, y_train, X_test, y_test):\n",
    "    param_names = [param.name for param in search_space]\n",
    "    params_dict = dict(zip(param_names, params))\n",
    "\n",
    "    if model_type == \"logistic_regression\":\n",
    "        penalty = params_dict[\"penalty\"]\n",
    "        solver = params_dict[\"solver\"]\n",
    "\n",
    "        if penalty == \"l1\" and solver not in [\"liblinear\", \"saga\"]:\n",
    "            return 1.0\n",
    "        if penalty == \"elasticnet\" and solver != \"saga\":\n",
    "            return 1.0\n",
    "        if penalty == \"none\" and solver not in [\"newton-cg\", \"lbfgs\", \"sag\", \"saga\"]:\n",
    "            return 1.0\n",
    "\n",
    "        if penalty != \"elasticnet\":\n",
    "            params_dict.pop(\"l1_ratio\", None)\n",
    "\n",
    "        model = LogisticRegression(**params_dict, random_state=42)\n",
    "\n",
    "    elif model_type == \"svc\":\n",
    "        model = SVC(**params_dict, probability=True, random_state=42)\n",
    "\n",
    "    elif model_type == \"random_forest\":\n",
    "        model = RandomForestClassifier(**params_dict, random_state=42)\n",
    "\n",
    "    elif model_type == \"xgboost\":\n",
    "        model = XGBClassifier(**params_dict, eval_metric='error', random_state=42)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model type.\")\n",
    "\n",
    "    try:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)  # Fixed: added y_proba calculation\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        loss = log_loss(y_test, y_proba)\n",
    "        print(f\"Model: {model_type}, F1 Score: {f1:.4f}, Log Loss: {loss:.4f}\")\n",
    "        return -f1\n",
    "    except:\n",
    "        return 1.0  # Return poor score for failed fits\n",
    "\n",
    "# Run Bayesian optimization for each model\n",
    "models = {\n",
    "    \"logistic_regression\": logistic_regression_space,\n",
    "    \"svc\": svc_space,\n",
    "    \"random_forest\": random_forest_space,\n",
    "    \"xgboost\": xgboost_space,\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Perform initial training and testing\n",
    "for model_type, search_space in models.items():\n",
    "    print(f\"\\nRunning optimization for {model_type}...\")\n",
    "    x_tuning_train, x_tuning_test, y_tuning_train, y_tuning_test = train_test_split(\n",
    "        x_llm,\n",
    "        y_feature_selection_tuning,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        shuffle=True\n",
    "    )\n",
    "    result = gp_minimize(\n",
    "            func=lambda params: objective_function(params, model_type, \n",
    "                                                x_tuning_train, y_tuning_train, x_tuning_test, y_tuning_test),\n",
    "            dimensions=search_space,\n",
    "            n_calls=50,\n",
    "            random_state=42,\n",
    "            verbose=False\n",
    "        )\n",
    "    # Save the results\n",
    "    results[model_type] = result\n",
    "    print(f\"Best hyperparameters for {model_type}: {result.x}\")\n",
    "    print(f\"Best F1 score for {model_type}: {-result.fun:.4f}\\n\")\n",
    "\n",
    "# Evaluate initial training and testing performance\n",
    "for model_type, result in results.items():\n",
    "    print(f\"\\nEvaluating initial {model_type} model...\")\n",
    "    param_names = [param.name for param in models[model_type]]\n",
    "    best_params = dict(zip(param_names, result.x))\n",
    "\n",
    "    # Handle special cases\n",
    "    if model_type == \"logistic_regression\":\n",
    "        if best_params.get(\"penalty\") != \"elasticnet\":\n",
    "            best_params.pop(\"l1_ratio\", None)\n",
    "        model = LogisticRegression(**best_params, random_state=42)\n",
    "    elif model_type == \"svc\":\n",
    "        model = SVC(**best_params, probability=True, random_state=42)\n",
    "    elif model_type == \"random_forest\":\n",
    "        model = RandomForestClassifier(**best_params, random_state=42)\n",
    "    elif model_type == \"xgboost\":\n",
    "        model = XGBClassifier(**best_params, eval_metric='error', random_state=42)\n",
    "\n",
    "    # Train on initial training data\n",
    "    model.fit(x_train[acc_features], y_train)\n",
    "    \n",
    "    # Get the F1 score, accuracy, and log loss on the training data\n",
    "    y_pred = model.predict(x_train[acc_features])\n",
    "    y_proba = model.predict_proba(x_train[acc_features])\n",
    "    f1 = f1_score(y_train, y_pred, average='weighted')\n",
    "    accuracy = accuracy_score(y_train, y_pred)  # Added: accuracy calculation\n",
    "    loss = log_loss(y_train, y_proba)\n",
    "\n",
    "    print(f\"Training - F1 Score: {f1:.4f}, Accuracy: {accuracy:.4f}, Log Loss: {loss:.4f}\")  # Added: accuracy to print\n",
    "\n",
    "    # Evaluate on testing data\n",
    "    for dataset_name, (X_data, y_data) in [(\"Testing\", (x_test[acc_features], y_test))]:\n",
    "        y_pred = model.predict(X_data)\n",
    "        y_proba = model.predict_proba(X_data)\n",
    "        f1 = f1_score(y_data, y_pred, average='weighted')\n",
    "        accuracy = accuracy_score(y_data, y_pred)  # Added: accuracy calculation\n",
    "        loss = log_loss(y_data, y_proba)\n",
    "        print(f\"{dataset_name} - F1 Score: {f1:.4f}, Accuracy: {accuracy:.4f}, Log Loss: {loss:.4f}\")  # Added: accuracy to print\n",
    "\n",
    "# Final training with the same hyperparameters and features\n",
    "for model_type, result in results.items():\n",
    "    print(f\"\\nTraining final {model_type} model...\")\n",
    "    param_names = [param.name for param in models[model_type]]\n",
    "    best_params = dict(zip(param_names, result.x))\n",
    "\n",
    "    # Handle special cases\n",
    "    if model_type == \"logistic_regression\":\n",
    "        if best_params.get(\"penalty\") != \"elasticnet\":\n",
    "            best_params.pop(\"l1_ratio\", None)\n",
    "        model = LogisticRegression(**best_params, random_state=42)\n",
    "    elif model_type == \"svc\":\n",
    "        model = SVC(**best_params, probability=True, random_state=42)\n",
    "    elif model_type == \"random_forest\":\n",
    "        model = RandomForestClassifier(**best_params, random_state=42)\n",
    "    elif model_type == \"xgboost\":\n",
    "        model = XGBClassifier(**best_params, eval_metric='error', random_state=42)\n",
    "\n",
    "    # Train on final training data\n",
    "    model.fit(x_final_train[acc_features], y_final_train)\n",
    "\n",
    "    print(f\"Final {model_type} model trained.\")\n",
    "    # Save the final model\n",
    "    with open(f'../pkl_files/{model_type}_model_f1.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating initial logistic_regression model...\n",
      "Training - F1 Score: 0.4675, Accuracy: 0.5397, Log Loss: 0.9895\n",
      "Testing - F1 Score: 0.4666, Accuracy: 0.5364, Log Loss: 0.9991\n",
      "\n",
      "Evaluating initial svc model...\n",
      "Training - F1 Score: 0.4725, Accuracy: 0.5531, Log Loss: 0.9532\n",
      "Testing - F1 Score: 0.4306, Accuracy: 0.5061, Log Loss: 1.0177\n",
      "\n",
      "Evaluating initial random_forest model...\n",
      "Training - F1 Score: 0.8414, Accuracy: 0.8451, Log Loss: 0.6064\n",
      "Testing - F1 Score: 0.4325, Accuracy: 0.4939, Log Loss: 1.0208\n",
      "\n",
      "Evaluating initial xgboost model...\n",
      "Training - F1 Score: 0.5513, Accuracy: 0.6039, Log Loss: 0.9482\n",
      "Testing - F1 Score: 0.4386, Accuracy: 0.5030, Log Loss: 1.0271\n",
      "Metrics saved to c:\\Users\\samue\\Documents\\GitHub\\Sports-betting-with-Machine-Learning\\model_performance_results\\f1_tune_openai4.1\\f1_tune_openai4.1.json\n",
      "Metrics also saved to c:\\Users\\samue\\Documents\\GitHub\\Sports-betting-with-Machine-Learning\\model_performance_results\\f1_tune_openai4.1\\f1_tune_openai4.1.csv\n",
      "Hyperparameters saved to c:\\Users\\samue\\Documents\\GitHub\\Sports-betting-with-Machine-Learning\\model_performance_results\\f1_tune_openai4.1\\hyperparams_openai4.1.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Dictionary to store all evaluation metrics\n",
    "evaluation_metrics = {\n",
    "    'model': [],\n",
    "    'f1_score': [],\n",
    "    'accuracy': [],\n",
    "    'log_loss': [],\n",
    "    'best_hyperparameters': []\n",
    "}\n",
    "\n",
    "# Collect metrics during the evaluation phase\n",
    "for model_type, result in results.items():\n",
    "    print(f\"\\nEvaluating initial {model_type} model...\")\n",
    "    param_names = [param.name for param in models[model_type]]\n",
    "    best_params = dict(zip(param_names, result.x))\n",
    "\n",
    "    # Handle special cases for model creation\n",
    "    if model_type == \"logistic_regression\":\n",
    "        if best_params.get(\"penalty\") != \"elasticnet\":\n",
    "            best_params.pop(\"l1_ratio\", None)\n",
    "        model = LogisticRegression(**best_params, random_state=42)\n",
    "    elif model_type == \"svc\":\n",
    "        model = SVC(**best_params, probability=True, random_state=42)\n",
    "    elif model_type == \"random_forest\":\n",
    "        model = RandomForestClassifier(**best_params, random_state=42)\n",
    "    elif model_type == \"xgboost\":\n",
    "        model = XGBClassifier(**best_params, eval_metric='error', random_state=42)\n",
    "\n",
    "    # Train on initial training data\n",
    "    model.fit(x_train[acc_features], y_train)\n",
    "    \n",
    "    # Get metrics on training data\n",
    "    y_pred_train = model.predict(x_train[acc_features])\n",
    "    y_proba_train = model.predict_proba(x_train[acc_features])\n",
    "    f1_train = f1_score(y_train, y_pred_train, average='weighted')\n",
    "    accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "    loss_train = log_loss(y_train, y_proba_train)\n",
    "\n",
    "    print(f\"Training - F1 Score: {f1_train:.4f}, Accuracy: {accuracy_train:.4f}, Log Loss: {loss_train:.4f}\")\n",
    "\n",
    "    # Get metrics on testing data\n",
    "    y_pred_test = model.predict(x_test[acc_features])\n",
    "    y_proba_test = model.predict_proba(x_test[acc_features])\n",
    "    f1_test = f1_score(y_test, y_pred_test, average='weighted')\n",
    "    accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "    loss_test = log_loss(y_test, y_proba_test)\n",
    "\n",
    "    print(f\"Testing - F1 Score: {f1_test:.4f}, Accuracy: {accuracy_test:.4f}, Log Loss: {loss_test:.4f}\")\n",
    "\n",
    "    # Store metrics in dictionary\n",
    "    evaluation_metrics['model'].append(model_type)\n",
    "    evaluation_metrics['f1_score'].append({\n",
    "        'training': f1_train,\n",
    "        'testing': f1_test,\n",
    "        'optimization_best': -result.fun\n",
    "    })\n",
    "    evaluation_metrics['accuracy'].append({\n",
    "        'training': accuracy_train,\n",
    "        'testing': accuracy_test\n",
    "    })\n",
    "    evaluation_metrics['log_loss'].append({\n",
    "        'training': loss_train,\n",
    "        'testing': loss_test\n",
    "    })\n",
    "    evaluation_metrics['best_hyperparameters'].append(best_params)\n",
    "\n",
    "# Create filename with LLM name\n",
    "llm_name = llm[selected_llm]\n",
    "filename = f\"f1_tune_{llm_name}\"\n",
    "\n",
    "# Save as JSON file\n",
    "project_root = Path.cwd().parent\n",
    "out_dir = project_root / \"model_performance_results\" / filename\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "json_filename = out_dir/f\"{filename}.json\"\n",
    "with open(json_filename, 'w') as f:\n",
    "    json.dump(evaluation_metrics, f, indent=4, default=str)  # default=str handles non-serializable objects\n",
    "print(f\"Metrics saved to {json_filename}\")\n",
    "\n",
    "# Alternative: Save as CSV file for easier analysis\n",
    "csv_filename = out_dir/f\"{filename}.csv\"\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Model': evaluation_metrics['model'],\n",
    "    'F1_Train': [m['training'] for m in evaluation_metrics['f1_score']],\n",
    "    'F1_Test': [m['testing'] for m in evaluation_metrics['f1_score']],\n",
    "    'F1_Best_Opt': [m['optimization_best'] for m in evaluation_metrics['f1_score']],\n",
    "    'Accuracy_Train': [m['training'] for m in evaluation_metrics['accuracy']],\n",
    "    'Accuracy_Test': [m['testing'] for m in evaluation_metrics['accuracy']],\n",
    "    'LogLoss_Train': [m['training'] for m in evaluation_metrics['log_loss']],\n",
    "    'LogLoss_Test': [m['testing'] for m in evaluation_metrics['log_loss']]\n",
    "})\n",
    "\n",
    "metrics_df.to_csv(csv_filename, index=False)\n",
    "print(f\"Metrics also saved to {csv_filename}\")\n",
    "\n",
    "# Alternative: Save hyperparameters separately\n",
    "project_root = Path.cwd().parent\n",
    "out_dir = project_root / \"model_performance_results\" / filename\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "hyperparams_filename = out_dir/f\"hyperparams_{llm_name}.json\"\n",
    "hyperparams_dict = {}\n",
    "for i, model_type in enumerate(evaluation_metrics['model']):\n",
    "    hyperparams_dict[model_type] = evaluation_metrics['best_hyperparameters'][i]\n",
    "\n",
    "with open(hyperparams_filename, 'w') as f:\n",
    "    json.dump(hyperparams_dict, f, indent=4, default=str)\n",
    "print(f\"Hyperparameters saved to {hyperparams_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calibration Branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport pandas as pd\\nimport numpy as np\\nfrom sklearn.model_selection import train_test_split\\nfrom skopt.space import Real, Integer, Categorical\\nfrom skopt import gp_minimize\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.svm import SVC\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom xgboost import XGBClassifier\\nfrom sklearn.metrics import accuracy_score, log_loss\\nfrom sklearn.calibration import calibration_curve\\nimport pickle\\n\\n# Define search spaces\\nlogistic_regression_space = [\\n    Categorical([\"l1\", \"l2\"], name=\"penalty\"),  \\n    Real(0.01, 10, name=\"C\"),\\n    Categorical([\"liblinear\", \"saga\"], name=\"solver\"),\\n    Real(0.0, 1.0, name=\"l1_ratio\"),\\n]\\n\\nsvc_space = [\\n    Categorical([\"linear\", \"rbf\"], name=\"kernel\"), \\n    Real(0.1, 10, name=\"C\", prior=\\'log-uniform\\'),   \\n    Categorical([\"scale\", \"auto\"], name=\"gamma\"),   \\n]\\nrandom_forest_space = [\\n    Integer(10, 200, name=\"n_estimators\"),\\n    Integer(2, 20, name=\"max_depth\"),\\n    Categorical([\"gini\", \"entropy\"], name=\"criterion\"),\\n]\\n\\nxgboost_space = [\\n    Integer(50, 100, name=\\'n_estimators\\'),\\n    Integer(3, 5, name=\\'max_depth\\'),\\n    Real(0.01, 0.1, name=\\'learning_rate\\'),\\n    Real(0.7, 0.9, name=\\'subsample\\'),\\n    Real(0.05, 0.1, name=\\'gamma\\'),\\n    Real(0, 1, name=\\'reg_lambda\\'),\\n    Real(0, 1, name=\\'reg_alpha\\'),\\n]\\n\\n# Define the objective function\\ndef objective_function(params, model_type, X_train, y_train, X_test, y_test, search_space):\\n    param_names = [param.name for param in search_space]\\n    params_dict = dict(zip(param_names, params))\\n\\n\\n    if model_type == \"logistic_regression\":\\n        penalty = params_dict.get(\"penalty\")\\n        solver = params_dict.get(\"solver\")\\n\\n        # Enhanced validation logic\\n        valid_combinations = {\\n            \"liblinear\": [\"l1\", \"l2\"],\\n            \"saga\": [\"l1\", \"l2\", \"elasticnet\", None]\\n        }\\n        if solver in valid_combinations:\\n            if penalty not in valid_combinations[solver]:\\n                return 1.0  # Penalize invalid combinations\\n        else:\\n            return 1.0  # Invalid solver\\n\\n        # Handle l1_ratio parameter\\n        if penalty != \"elasticnet\":\\n            params_dict.pop(\"l1_ratio\", None)\\n\\n        try:\\n            model = LogisticRegression(**params_dict, max_iter=1000)\\n        except ValueError:\\n            return 1.0  # Catch any unexpected parameter errors\\n\\n    elif model_type == \"svc\":\\n        model = SVC(**params_dict, probability=True)\\n\\n    elif model_type == \"random_forest\":\\n        model = RandomForestClassifier(**params_dict)\\n\\n    elif model_type == \"xgboost\":\\n        model = XGBClassifier(**params_dict, use_label_encoder=False, eval_metric=\\'logloss\\')\\n\\n    else:\\n        raise ValueError(\"Invalid model type.\")\\n\\n    model.fit(X_train, y_train)\\n    y_proba = model.predict_proba(X_test)\\n    loss = log_loss(y_test, y_proba)\\n    return loss  # Minimize log loss\\n\\n# Function to compute Class-Wise Expected Calibration Error (ECE)\\ndef compute_classwise_ece(y_true, y_proba, bins=10):\\n    \"\"\"\\n    Compute ECE for each class separately.\\n    Args:\\n        y_true: True labels (array-like).\\n        y_proba: Predicted probabilities (array-like, shape [n_samples, n_classes]).\\n        bins: Number of bins for calibration curve.\\n    Returns:\\n        classwise_ece: Dictionary mapping class index to its ECE.\\n    \"\"\"\\n    classwise_ece = {}\\n    n_classes = y_proba.shape[1]\\n\\n    for class_idx in range(n_classes):\\n        # One-vs-rest for the current class\\n        y_true_binary = (y_true == class_idx).astype(int)\\n        y_proba_class = y_proba[:, class_idx]\\n\\n        # Compute calibration curve\\n        prob_true, prob_pred = calibration_curve(y_true_binary, y_proba_class, n_bins=bins)\\n\\n        # Compute ECE for this class\\n        ece = np.mean(np.abs(prob_true - prob_pred))\\n        classwise_ece[class_idx] = ece\\n\\n    return classwise_ece\\n\\n# Function to compute Total Class-Wise ECE (Weighted Average)\\ndef compute_total_classwise_ece(y_true, y_proba, bins=10):\\n    \"\"\"\\n    Compute the total class-wise ECE as a weighted average.\\n    Args:\\n        y_true: True labels (array-like).\\n        y_proba: Predicted probabilities (array-like, shape [n_samples, n_classes]).\\n        bins: Number of bins for calibration curve.\\n    Returns:\\n        total_ece: Weighted average of class-wise ECEs.\\n    \"\"\"\\n    classwise_ece = compute_classwise_ece(y_true, y_proba, bins)\\n    n_classes = len(classwise_ece)\\n    total_weight = 0\\n    weighted_sum = 0\\n\\n    for class_idx in range(n_classes):\\n        # Weight is the number of samples in the class\\n        weight = np.sum(y_true == class_idx)\\n        weighted_sum += weight * classwise_ece[class_idx]\\n        total_weight += weight\\n\\n    total_ece = weighted_sum / total_weight if total_weight > 0 else 0\\n    return total_ece\\n\\n# Run Bayesian optimization for each model\\nmodels = {\\n    \"logistic_regression\": logistic_regression_space,\\n    \"svc\": svc_space,\\n    \"random_forest\": random_forest_space,\\n    \"xgboost\": xgboost_space,\\n}\\n\\nresults = {}\\n\\n# Perform initial training and testing\\n\\nfor model_type, search_space in models.items():\\n    print(f\"\\nRunning optimization for {model_type}...\")\\n    x_tuning_train, x_tuning_test, y_tuning_train, y_tuning_test = train_test_split(\\n        x_tuning[calib_features],\\n        y_feature_selection_tuning,\\n        test_size=0.2,\\n        random_state=42,\\n        shuffle=True\\n    )\\n    result = gp_minimize(\\n        func=lambda params: objective_function(\\n            params, \\n            model_type, \\n            x_tuning_train, y_tuning_train, \\n            x_tuning_test, y_tuning_test,\\n            search_space  # Pass current search space\\n        ),\\n        dimensions=search_space,\\n        n_calls=50,\\n        random_state=42,\\n        verbose=False\\n    )\\n    # Save the results\\n    results[model_type] = result\\n    print(f\"Best hyperparameters for {model_type}: {result.x}\")\\n    print(f\"Best log loss for {model_type}: {result.fun:.4f}\\n\")\\n\\n# Evaluate initial training and testing performance\\nfor model_type, result in results.items():\\n    print(f\"\\nEvaluating initial {model_type} model...\")\\n    param_names = [param.name for param in models[model_type]]\\n    best_params = dict(zip(param_names, result.x))\\n\\n    # Handle special cases\\n    if model_type == \"logistic_regression\":\\n        # Post-hoc validation of best parameters\\n        penalty = best_params.get(\"penalty\")\\n        solver = best_params.get(\"solver\")\\n\\n        valid_combinations = {\\n            \"liblinear\": [\"l1\", \"l2\"],\\n            \"saga\": [\"l1\", \"l2\", \"elasticnet\"]\\n        }\\n\\n        if solver not in valid_combinations or penalty not in valid_combinations[solver]:\\n            raise ValueError(f\"Invalid combination: penalty={penalty}, solver={solver}\")\\n        if penalty != \"elasticnet\":\\n            best_params.pop(\"l1_ratio\", None)\\n        model = LogisticRegression(**best_params)\\n    elif model_type == \"svc\":\\n        model = SVC(**best_params, probability=True)\\n    elif model_type == \"random_forest\":\\n        model = RandomForestClassifier(**best_params)\\n    elif model_type == \"xgboost\":\\n        model = XGBClassifier(**best_params, eval_metric=\\'logloss\\')\\n\\n    # Train on initial training data\\n    model.fit(x_train[calib_features], y_train)\\n\\n    # Get metrics on the training data\\n    y_pred = model.predict(x_train[calib_features])\\n    y_proba = model.predict_proba(x_train[calib_features])\\n    accuracy = accuracy_score(y_train, y_pred)\\n    loss = log_loss(y_train, y_proba)\\n    total_ece = compute_total_classwise_ece(y_train, y_proba)\\n    print(f\"Training Accuracy: {accuracy:.4f}, Training Log Loss: {loss:.4f}, Training Total ECE: {total_ece:.4f}\")\\n\\n    # Evaluate on testing data\\n    for dataset_name, (X_data, y_data) in [(\"Testing\", (x_test[calib_features], y_test))]:\\n        y_pred = model.predict(X_data)\\n        y_proba = model.predict_proba(X_data)\\n        accuracy = accuracy_score(y_data, y_pred)\\n        loss = log_loss(y_data, y_proba)\\n        total_ece = compute_total_classwise_ece(y_data, y_proba)\\n        print(f\"{dataset_name} Accuracy: {accuracy:.4f}, {dataset_name} Log Loss: {loss:.4f}, {dataset_name} Total ECE: {total_ece:.4f}\")\\n\\n# Final training with the same hyperparameters and features\\nfor model_type, result in results.items():\\n    print(f\"\\nTraining final {model_type} model...\")\\n    param_names = [param.name for param in models[model_type]]\\n    best_params = dict(zip(param_names, result.x))\\n\\n    # Handle special cases\\n    if model_type == \"logistic_regression\":\\n        if best_params.get(\"penalty\") != \"elasticnet\":\\n            best_params.pop(\"l1_ratio\", None)\\n        model = LogisticRegression(**best_params)\\n    elif model_type == \"svc\":\\n        model = SVC(**best_params, probability=True)\\n    elif model_type == \"random_forest\":\\n        model = RandomForestClassifier(**best_params)\\n    elif model_type == \"xgboost\":\\n        model = XGBClassifier(**best_params, eval_metric=\\'logloss\\')\\n\\n    # Train on final training data\\n    model.fit(x_final_train[calib_features], y_final_train)\\n\\n    print(model.feature_names_in_)\\n\\n    print(f\"Final {model_type} model trained.\")\\n    # Save the final model\\n    with open(f\\'calibration_pkl_files/{model_type}_model_calib_{llm[selected_llm]}.pkl\\', \\'wb\\') as f:\\n        pickle.dump(model, f)\\n\\n\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt import gp_minimize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.calibration import calibration_curve\n",
    "import pickle\n",
    "\n",
    "# Define search spaces\n",
    "logistic_regression_space = [\n",
    "    Categorical([\"l1\", \"l2\"], name=\"penalty\"),  \n",
    "    Real(0.01, 10, name=\"C\"),\n",
    "    Categorical([\"liblinear\", \"saga\"], name=\"solver\"),\n",
    "    Real(0.0, 1.0, name=\"l1_ratio\"),\n",
    "]\n",
    "\n",
    "svc_space = [\n",
    "    Categorical([\"linear\", \"rbf\"], name=\"kernel\"), \n",
    "    Real(0.1, 10, name=\"C\", prior='log-uniform'),   \n",
    "    Categorical([\"scale\", \"auto\"], name=\"gamma\"),   \n",
    "]\n",
    "random_forest_space = [\n",
    "    Integer(10, 200, name=\"n_estimators\"),\n",
    "    Integer(2, 20, name=\"max_depth\"),\n",
    "    Categorical([\"gini\", \"entropy\"], name=\"criterion\"),\n",
    "]\n",
    "\n",
    "xgboost_space = [\n",
    "    Integer(50, 100, name='n_estimators'),\n",
    "    Integer(3, 5, name='max_depth'),\n",
    "    Real(0.01, 0.1, name='learning_rate'),\n",
    "    Real(0.7, 0.9, name='subsample'),\n",
    "    Real(0.05, 0.1, name='gamma'),\n",
    "    Real(0, 1, name='reg_lambda'),\n",
    "    Real(0, 1, name='reg_alpha'),\n",
    "]\n",
    "\n",
    "# Define the objective function\n",
    "def objective_function(params, model_type, X_train, y_train, X_test, y_test, search_space):\n",
    "    param_names = [param.name for param in search_space]\n",
    "    params_dict = dict(zip(param_names, params))\n",
    "\n",
    "\n",
    "    if model_type == \"logistic_regression\":\n",
    "        penalty = params_dict.get(\"penalty\")\n",
    "        solver = params_dict.get(\"solver\")\n",
    "\n",
    "        # Enhanced validation logic\n",
    "        valid_combinations = {\n",
    "            \"liblinear\": [\"l1\", \"l2\"],\n",
    "            \"saga\": [\"l1\", \"l2\", \"elasticnet\", None]\n",
    "        }\n",
    "        if solver in valid_combinations:\n",
    "            if penalty not in valid_combinations[solver]:\n",
    "                return 1.0  # Penalize invalid combinations\n",
    "        else:\n",
    "            return 1.0  # Invalid solver\n",
    "\n",
    "        # Handle l1_ratio parameter\n",
    "        if penalty != \"elasticnet\":\n",
    "            params_dict.pop(\"l1_ratio\", None)\n",
    "\n",
    "        try:\n",
    "            model = LogisticRegression(**params_dict, max_iter=1000)\n",
    "        except ValueError:\n",
    "            return 1.0  # Catch any unexpected parameter errors\n",
    "\n",
    "    elif model_type == \"svc\":\n",
    "        model = SVC(**params_dict, probability=True)\n",
    "\n",
    "    elif model_type == \"random_forest\":\n",
    "        model = RandomForestClassifier(**params_dict)\n",
    "\n",
    "    elif model_type == \"xgboost\":\n",
    "        model = XGBClassifier(**params_dict, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model type.\")\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_proba = model.predict_proba(X_test)\n",
    "    loss = log_loss(y_test, y_proba)\n",
    "    return loss  # Minimize log loss\n",
    "\n",
    "# Function to compute Class-Wise Expected Calibration Error (ECE)\n",
    "def compute_classwise_ece(y_true, y_proba, bins=10):\n",
    "    \"\"\"\n",
    "    Compute ECE for each class separately.\n",
    "    Args:\n",
    "        y_true: True labels (array-like).\n",
    "        y_proba: Predicted probabilities (array-like, shape [n_samples, n_classes]).\n",
    "        bins: Number of bins for calibration curve.\n",
    "    Returns:\n",
    "        classwise_ece: Dictionary mapping class index to its ECE.\n",
    "    \"\"\"\n",
    "    classwise_ece = {}\n",
    "    n_classes = y_proba.shape[1]\n",
    "    \n",
    "    for class_idx in range(n_classes):\n",
    "        # One-vs-rest for the current class\n",
    "        y_true_binary = (y_true == class_idx).astype(int)\n",
    "        y_proba_class = y_proba[:, class_idx]\n",
    "        \n",
    "        # Compute calibration curve\n",
    "        prob_true, prob_pred = calibration_curve(y_true_binary, y_proba_class, n_bins=bins)\n",
    "        \n",
    "        # Compute ECE for this class\n",
    "        ece = np.mean(np.abs(prob_true - prob_pred))\n",
    "        classwise_ece[class_idx] = ece\n",
    "    \n",
    "    return classwise_ece\n",
    "\n",
    "# Function to compute Total Class-Wise ECE (Weighted Average)\n",
    "def compute_total_classwise_ece(y_true, y_proba, bins=10):\n",
    "    \"\"\"\n",
    "    Compute the total class-wise ECE as a weighted average.\n",
    "    Args:\n",
    "        y_true: True labels (array-like).\n",
    "        y_proba: Predicted probabilities (array-like, shape [n_samples, n_classes]).\n",
    "        bins: Number of bins for calibration curve.\n",
    "    Returns:\n",
    "        total_ece: Weighted average of class-wise ECEs.\n",
    "    \"\"\"\n",
    "    classwise_ece = compute_classwise_ece(y_true, y_proba, bins)\n",
    "    n_classes = len(classwise_ece)\n",
    "    total_weight = 0\n",
    "    weighted_sum = 0\n",
    "    \n",
    "    for class_idx in range(n_classes):\n",
    "        # Weight is the number of samples in the class\n",
    "        weight = np.sum(y_true == class_idx)\n",
    "        weighted_sum += weight * classwise_ece[class_idx]\n",
    "        total_weight += weight\n",
    "    \n",
    "    total_ece = weighted_sum / total_weight if total_weight > 0 else 0\n",
    "    return total_ece\n",
    "\n",
    "# Run Bayesian optimization for each model\n",
    "models = {\n",
    "    \"logistic_regression\": logistic_regression_space,\n",
    "    \"svc\": svc_space,\n",
    "    \"random_forest\": random_forest_space,\n",
    "    \"xgboost\": xgboost_space,\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Perform initial training and testing\n",
    "    \n",
    "for model_type, search_space in models.items():\n",
    "    print(f\"\\nRunning optimization for {model_type}...\")\n",
    "    x_tuning_train, x_tuning_test, y_tuning_train, y_tuning_test = train_test_split(\n",
    "        x_tuning[calib_features],\n",
    "        y_feature_selection_tuning,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        shuffle=True\n",
    "    )\n",
    "    result = gp_minimize(\n",
    "        func=lambda params: objective_function(\n",
    "            params, \n",
    "            model_type, \n",
    "            x_tuning_train, y_tuning_train, \n",
    "            x_tuning_test, y_tuning_test,\n",
    "            search_space  # Pass current search space\n",
    "        ),\n",
    "        dimensions=search_space,\n",
    "        n_calls=50,\n",
    "        random_state=42,\n",
    "        verbose=False\n",
    "    )\n",
    "    # Save the results\n",
    "    results[model_type] = result\n",
    "    print(f\"Best hyperparameters for {model_type}: {result.x}\")\n",
    "    print(f\"Best log loss for {model_type}: {result.fun:.4f}\\n\")\n",
    "\n",
    "# Evaluate initial training and testing performance\n",
    "for model_type, result in results.items():\n",
    "    print(f\"\\nEvaluating initial {model_type} model...\")\n",
    "    param_names = [param.name for param in models[model_type]]\n",
    "    best_params = dict(zip(param_names, result.x))\n",
    "\n",
    "    # Handle special cases\n",
    "    if model_type == \"logistic_regression\":\n",
    "        # Post-hoc validation of best parameters\n",
    "        penalty = best_params.get(\"penalty\")\n",
    "        solver = best_params.get(\"solver\")\n",
    "        \n",
    "        valid_combinations = {\n",
    "            \"liblinear\": [\"l1\", \"l2\"],\n",
    "            \"saga\": [\"l1\", \"l2\", \"elasticnet\"]\n",
    "        }\n",
    "        \n",
    "        if solver not in valid_combinations or penalty not in valid_combinations[solver]:\n",
    "            raise ValueError(f\"Invalid combination: penalty={penalty}, solver={solver}\")\n",
    "        if penalty != \"elasticnet\":\n",
    "            best_params.pop(\"l1_ratio\", None)\n",
    "        model = LogisticRegression(**best_params)\n",
    "    elif model_type == \"svc\":\n",
    "        model = SVC(**best_params, probability=True)\n",
    "    elif model_type == \"random_forest\":\n",
    "        model = RandomForestClassifier(**best_params)\n",
    "    elif model_type == \"xgboost\":\n",
    "        model = XGBClassifier(**best_params, eval_metric='logloss')\n",
    "\n",
    "    # Train on initial training data\n",
    "    model.fit(x_train[calib_features], y_train)\n",
    "\n",
    "    # Get metrics on the training data\n",
    "    y_pred = model.predict(x_train[calib_features])\n",
    "    y_proba = model.predict_proba(x_train[calib_features])\n",
    "    accuracy = accuracy_score(y_train, y_pred)\n",
    "    loss = log_loss(y_train, y_proba)\n",
    "    total_ece = compute_total_classwise_ece(y_train, y_proba)\n",
    "    print(f\"Training Accuracy: {accuracy:.4f}, Training Log Loss: {loss:.4f}, Training Total ECE: {total_ece:.4f}\")\n",
    "\n",
    "    # Evaluate on testing data\n",
    "    for dataset_name, (X_data, y_data) in [(\"Testing\", (x_test[calib_features], y_test))]:\n",
    "        y_pred = model.predict(X_data)\n",
    "        y_proba = model.predict_proba(X_data)\n",
    "        accuracy = accuracy_score(y_data, y_pred)\n",
    "        loss = log_loss(y_data, y_proba)\n",
    "        total_ece = compute_total_classwise_ece(y_data, y_proba)\n",
    "        print(f\"{dataset_name} Accuracy: {accuracy:.4f}, {dataset_name} Log Loss: {loss:.4f}, {dataset_name} Total ECE: {total_ece:.4f}\")\n",
    "\n",
    "# Final training with the same hyperparameters and features\n",
    "for model_type, result in results.items():\n",
    "    print(f\"\\nTraining final {model_type} model...\")\n",
    "    param_names = [param.name for param in models[model_type]]\n",
    "    best_params = dict(zip(param_names, result.x))\n",
    "\n",
    "    # Handle special cases\n",
    "    if model_type == \"logistic_regression\":\n",
    "        if best_params.get(\"penalty\") != \"elasticnet\":\n",
    "            best_params.pop(\"l1_ratio\", None)\n",
    "        model = LogisticRegression(**best_params)\n",
    "    elif model_type == \"svc\":\n",
    "        model = SVC(**best_params, probability=True)\n",
    "    elif model_type == \"random_forest\":\n",
    "        model = RandomForestClassifier(**best_params)\n",
    "    elif model_type == \"xgboost\":\n",
    "        model = XGBClassifier(**best_params, eval_metric='logloss')\n",
    "\n",
    "    # Train on final training data\n",
    "    model.fit(x_final_train[calib_features], y_final_train)\n",
    "\n",
    "    print(model.feature_names_in_)\n",
    "\n",
    "    print(f\"Final {model_type} model trained.\")\n",
    "    # Save the final model\n",
    "    with open(f'calibration_pkl_files/{model_type}_model_calib_{llm[selected_llm]}.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Betting Simulation</h1>\n",
    "Start with a 10000 bankroll\n",
    "<ul>\n",
    "<li>Fixed betting, keep placing $100 bets as long as there is a value bet identified</li>\n",
    "<li>One-eighth Kelly betting, keep placing bets based on the Kelly criterion. Depends on the size of the current bankroll</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to print the stats for each case. Betting simulation data points are saved into another file for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running betting simulation for SVC model...\n",
      "Bankroll after betting on F1 model: 12111.530854610202\n",
      "Win rate: 0.2978723404255319\n",
      "Number of bets: 325\n",
      "{'home': 160, 'draw': 129, 'away': 36, 'none': 4}\n",
      "Running betting simulation for Random Forest model...\n",
      "Bankroll after betting on F1 model: 37948.340454927245\n",
      "Win rate: 0.3130699088145897\n",
      "Number of bets: 325\n",
      "{'home': 144, 'draw': 127, 'away': 54, 'none': 4}\n",
      "Running betting simulation for Logistic Regression model...\n",
      "Bankroll after betting on F1 model: 16001.868843298947\n",
      "Win rate: 0.2948328267477204\n",
      "Number of bets: 326\n",
      "{'home': 130, 'draw': 144, 'away': 52, 'none': 3}\n",
      "Running betting simulation for XGBoost model...\n",
      "Bankroll after betting on F1 model: 14074.129111824084\n",
      "Win rate: 0.25835866261398177\n",
      "Number of bets: 326\n",
      "{'home': 120, 'draw': 190, 'away': 16, 'none': 3}\n"
     ]
    }
   ],
   "source": [
    "# Run each model from F1 feature selection against the betting simulation data (final test data)\n",
    "# This is after the final model has been fitted\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def what_to_bet(home_odds, draw_odds, away_odds, home, draw, away):\n",
    "    if home_odds > home:\n",
    "        return 'home'\n",
    "    elif draw_odds > draw:\n",
    "        return 'draw'\n",
    "    elif away_odds > away:\n",
    "        return 'away'\n",
    "    return None\n",
    "\n",
    "def amount_to_bet(bankroll, bookmaker_odds, model_odds):\n",
    "    p = 1/model_odds\n",
    "    kelly_criterion= p-((1-p)/(bookmaker_odds-1))\n",
    "    \n",
    "    if kelly_criterion > 0:\n",
    "        return bankroll * kelly_criterion/8 #one eighth of the bankroll max bet\n",
    "    else: \n",
    "        return 0\n",
    "\n",
    "def update_bankroll(amount_to_bet, model_result, actual_result, bookmaker_odds):\n",
    "    if model_result == actual_result:\n",
    "        pnl = amount_to_bet*(bookmaker_odds-1)\n",
    "    else:\n",
    "        pnl = amount_to_bet*-1\n",
    "    \n",
    "    return pnl\n",
    "\n",
    "data_to_save = {\n",
    "    'kelly_f1': {  \n",
    "        \"SVC\": [],\n",
    "        \"Random Forest\": [],\n",
    "        \"Logistic Regression\": [],\n",
    "        \"XGBoost\": []\n",
    "    }   \n",
    "}\n",
    "\n",
    "models_config = [\n",
    "    {\n",
    "        'name': 'SVC',\n",
    "        'f1_file': f'../pkl_files/svc_model_f1.pkl',\n",
    "        'f1_features': [acc_features, acc_features]\n",
    "    },\n",
    "    {\n",
    "        'name': 'Random Forest',\n",
    "        'f1_file': f'../pkl_files/random_forest_model_f1.pkl',\n",
    "        'f1_features': [acc_features, acc_features]\n",
    "    },\n",
    "    {\n",
    "        'name': 'Logistic Regression',\n",
    "        'f1_file': f'../pkl_files/logistic_regression_model_f1.pkl',\n",
    "        'f1_features': [acc_features, acc_features]\n",
    "    },\n",
    "    {\n",
    "        'name': 'XGBoost',\n",
    "        'f1_file': f'../pkl_files/xgboost_model_f1.pkl',\n",
    "        'f1_features': [acc_features, acc_features]\n",
    "    }\n",
    "]\n",
    "\n",
    "for model_config in models_config:\n",
    "    \n",
    "    print(f\"Running betting simulation for {model_config['name']} model...\")\n",
    "\n",
    "    model_name = model_config['name']\n",
    "    f1_file = model_config['f1_file']\n",
    "\n",
    "    # Load F1 model from the pickle file\n",
    "    with open(f1_file, 'rb') as f:\n",
    "        f1_final_model = pickle.load(f)\n",
    "\n",
    "    with open('../dataset/epl_combined/epl_combined_2023.json', 'r') as f:\n",
    "        data_2023 = json.load(f)\n",
    "\n",
    "    data_2023_odds = pd.json_normalize(data_2023)\n",
    "    data_2023_odds = data_2023_odds[[\"home_odds\", \"draw_odds\", \"away_odds\", \"actual_winner\"]]\n",
    "    #convert home_odds, draw_odds, away_odds to float\n",
    "    data_2023_odds[\"home_odds\"] = data_2023_odds[\"home_odds\"].astype(float)\n",
    "    data_2023_odds[\"draw_odds\"] = data_2023_odds[\"draw_odds\"].astype(float)\n",
    "    data_2023_odds[\"away_odds\"] = data_2023_odds[\"away_odds\"].astype(float)\n",
    "\n",
    "    # F1 model predictions\n",
    "    f1_y_probs = f1_final_model.predict_proba(x_betting_simulation[acc_features])\n",
    "    f1_y_classes = f1_final_model.classes_\n",
    "    f1_y_probs = 1 / f1_y_probs\n",
    "    f1_y_implied_odds_df = pd.DataFrame(f1_y_probs, columns=f1_y_classes)\n",
    "\n",
    "    #Concatenate the actual odds and the implied odds\n",
    "    f1_df = pd.concat([data_2023_odds, f1_y_implied_odds_df], axis=1)\n",
    "\n",
    "    # Rename the columns to match the betting simulation data\n",
    "    f1_df.columns = ['home_odds', 'draw_odds', 'away_odds', 'actual_winner', 'home', 'draw', 'away']\n",
    "\n",
    "    #convert to dictionary\n",
    "    f1_dict = f1_df.to_dict(orient='records')\n",
    "\n",
    "    starting_bankroll = 10000\n",
    "\n",
    "    #Kelly betting for F1 model\n",
    "    bankroll = starting_bankroll\n",
    "    win_count = 0\n",
    "    bet_count = 0\n",
    "    betting_decisions = {'home': 0, 'draw': 0, 'away': 0, 'none': 0}\n",
    "\n",
    "    for match in f1_dict:\n",
    "\n",
    "        home_odds = match['home_odds']\n",
    "        draw_odds = match['draw_odds']\n",
    "        away_odds = match['away_odds']\n",
    "        home = match['home']\n",
    "        draw = match['draw']\n",
    "        away = match['away']\n",
    "        result = match['actual_winner']\n",
    "\n",
    "        betting_decision = what_to_bet(home_odds, draw_odds, away_odds, home, draw, away)\n",
    "        if betting_decision is None:\n",
    "            betting_decisions['none'] += 1\n",
    "        else:\n",
    "            betting_decisions[betting_decision] += 1\n",
    "\n",
    "        if betting_decision is not None:\n",
    "            bet_count += 1\n",
    "            odds_to_use = match[betting_decision+ '_odds']\n",
    "            amount = amount_to_bet(bankroll, odds_to_use,match[betting_decision])\n",
    "            bankroll += update_bankroll(amount, betting_decision, result, odds_to_use)\n",
    "            if betting_decision == result:\n",
    "                win_count += 1\n",
    "\n",
    "        #add bankroll to the data_to_save dict\n",
    "        data_to_save['kelly_f1'][model_config['name']].append(bankroll)\n",
    "\n",
    "    print(f\"Bankroll after betting on F1 model: {bankroll}\")\n",
    "    print(f\"Win rate: {win_count / len(f1_dict)}\")\n",
    "    print(f\"Number of bets: {bet_count}\")\n",
    "    print(betting_decisions)\n",
    "\n",
    "\n",
    "\n",
    "# Save the results to a JSON file\n",
    "with open(f'../betting_simulation_results/betting_simulation_results_{llm[selected_llm]}.json', 'w') as f:\n",
    "    json.dump(data_to_save, f, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
